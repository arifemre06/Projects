{
 "cells": [
  {
   "attachments": {
    "standardization.JPG": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/4RDcRXhpZgAATU0AKgAAAAgABAE7AAIAAAAGAAAISodpAAQAAAABAAAIUJydAAEAAAAMAAAQyOocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEZleXphAAAFkAMAAgAAABQAABCekAQAAgAAABQAABCykpEAAgAAAAM4MwAAkpIAAgAAAAM4MwAA6hwABwAACAwAAAiSAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMjowNDoxNyAxMDoyMjowOAAyMDIyOjA0OjE3IDEwOjIyOjA4AAAARgBlAHkAegBhAAAA/+ELGGh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iLz48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjItMDQtMTdUMTA6MjI6MDguODI5PC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyI+PGRjOmNyZWF0b3I+PHJkZjpTZXEgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj48cmRmOmxpPkZleXphPC9yZGY6bGk+PC9yZGY6U2VxPg0KCQkJPC9kYzpjcmVhdG9yPjwvcmRmOkRlc2NyaXB0aW9uPjwvcmRmOlJERj48L3g6eG1wbWV0YT4NCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgPD94cGFja2V0IGVuZD0ndyc/Pv/bAEMABwUFBgUEBwYFBggHBwgKEQsKCQkKFQ8QDBEYFRoZGBUYFxseJyEbHSUdFxgiLiIlKCkrLCsaIC8zLyoyJyorKv/bAEMBBwgICgkKFAsLFCocGBwqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKv/AABEIAVMB8wMBIgACEQEDEQH/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2gAMAwEAAhEDEQA/APpGsjX/ABFDoEdqrWl1fXd7N5FrZ2iqZJm2ljjcVUAKpJLEDA/CteuV8ZRaZd6hodnfX95pd/JcO+m39syr5UwQ5Q7wVO5Cw2lTkA9DigCey8a2MsepjVbS80afSoRcXcF6illhIbEimNnVl+Vh8pJypBFdHXkvil72XRvFvhzxDdWWufZdDN5FfJbiGWM5cKkwU7c8bgRtB+b5RVvxJpmn3via4sLfR9Pv4NH0yEmPWNSNvZ2aMXKvGixuS+FOXOMBVAPWm7KKl/WgW1a/rp/men0V5N4d0208VzeEYdedtTtj4aaaSKSZniuHEkShnBP7zGTjdnnnrUotbR7bRPD+qTyLoT67fWxiknYLKEaTyLZjnlODhScHYq89KXbz/R2Htr/W1z0iPU4ZdcuNKVZPPt7eK4diBtKyNIqgHOc5ibPHcVcryO8TRPDureNbXSJrmO1Wx063e3sLoRm1lllmQRozZEIO9ScY2hyRzUmj239ieNNQ06Ow0nRzLoM88llpWoPPvYOgWSVWRMPgsAwBJG7J4FD2+T/BNhbW3p+h6xWXNr0EWpXliltdTz2cdvJIsMYbKzOyKRz22MW9AO9efweHNKtLfwdo9+0h0zVI3uL43EzN/aN2IU8tJWJ+YEb22k4PlqMYAFRfY9N0rVvFtpoE5FtDPo6mGOUlbVvtB3RJz8q4IO0cDeRgdKppJtPoKL5kn3PWaK821TSNI8YeM5rDTLeKC20+4WTV9SiYq8kwwwto2B69DIR0B29Scc7448q7s/Fuu29jZs+nyyQR6rqmqtHPZyoigLaxpGdnzYIBZS7HngipWv8AXp/mO2p69catbwzXkCCW4urO2W5ktoYyXZG3hdvYkmNgBnt2yKtxvvjV9rLuAO1hgj615zremWCeIvGGpRQxrenwxE4uFOHywuQSCD3CJ+Qp1tpOj+I/FOsReLlS5NhbWxsre5kISCAxBjOgyAGMnmAyDkbAMijq1/XX/IGtE/66f5no1FeR+FtJtfF2qabH4iefVrOPRWMUd25KXSC6kWKWRejt5aqQTn72evNdb4Qgto9DsYLi9mU2Wq31vZJJdNmQJNOixnJzIFjBwDnGwHtmmtVf+t2v0F/X3q519FFFIAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACqupaXYaxZNZ6tZW99auQWhuYlkQkcg4IIq1RQBmWXhnQtO02407T9G0+2srkET20NsixygjB3KBhsjjntUUnhDw3MLMTeH9LkFiAtrus4z5AByAnHy888d62KKAKdnpGm6ewaw0+1tWUMAYYFQgM25hwO7cn1PNJdaNpd9YS2N7p1pcWkzmSW3lgVo3YncWKkYJzzn15q7RQBmW/hvQ7SxlsrXRtPgtJovJlgjtUVJI8sdjKBgjLMcHj5j6morPwj4c04xmw0DS7YxFzGYbONChddr4wOMrwfUcVsUUXYFO+0jTdT03+z9R0+1u7LAH2aeFXj46fKRjjHHpUdroGj2Nr9mstKsbeDCDyordFXCsWXgDHDEkehJNaFFAGFceBvCV3dSXN34X0We4mcvJLLp8TO7E5LElckk96nn8K+HrrU21G60LTZr512tcyWkbSMNu3BYjP3ePpxWtRQBmHw1oRmglOjaeZLe3+ywv9lTMUOCPLU44TBI2jjBNGqeGtD1vyf7Z0bT9Q8gYi+1WqS+WPRdwOOg6eladFAES2lulx9oS3iWbyxF5gQBtgOQueuM9qjGm2KtEVs7cGGV54iIl+SR925xxwx3tkjk7j6mrNFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFcR4o8VeIdC8aaVpVjZWN9b61HNFZq26N4p0CtukfcR5e0sTtXdxxnNHWwHb0VTsHvodGhk1w25vUi3XJs1by9wHOwHLY9utct/wt3wael/e/wDgou//AI1QB2tFZ+ia7p/iLTRf6RLJLbMxQNJBJEcjr8rqD+lYN544Wx8S+Ibe4WFNL0DTY7q6uCTv819zBPT7ig+uWFJu24LXY66isjwpqd5rXhHTNU1OCO3ur22Sd4Y87U3DcBzz0I/GteqaadmJO6ugooopDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAorC8aXGk2vg/UJPEV1JbaaIwJ2ik8t3GR8gPXLfd4IPPBHWs/wJ4Pg8N29zevEYb3USrvB9peZLVAPliVmJzjqzfxMSeBtAEB1tFcD8QbV7jVLQpB42lxCc/8I5drDEPm/jBdct+fFYWg2U0PiGwf7H8ThidMnUdRSS3HPWRfNOU9eDxRHUHoet1wGo/DnU9We01K+8Tl/EFjdpNa6gtkoSCIZBiWHdj5gx3NnJIXsoA7+ud+2+NP+gBoP/g8m/8AkSjrcfSxvW8Jt7WKEyyTGNApkkOWfAxkn1NSVzv23xp/0ANB/wDB5N/8iUfbfGn/AEANB/8AB5N/8iUCOiJCgknAHUmvFbLwBffErw/4h1O48QPZ2HiG/e4tbdLUEFYn8uKR23BpF2RgquQuSG54r0j7b40/6AGg/wDg8m/+RKPtvjT/AKAGg/8Ag8m/+RKVtbjvY1tIsH0zSbezlupLuSJMNPIAC57nA4UegHAGBVys7SZ9am83+3NPsLPGPK+x3z3G/rnduij244xjOcnpjnRp7iWgUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFHV9D0rX7MWuuada6hArb1juoVkVWwRuAI4OCeRzzVHw74Q0nwuZTpEc6+ZFHB++uHl2RR7tkalySFXe2PrjoBjcooAKKKKACiiigArlfFPxH8O+ELr7Jqc8814ITcPbWcDTSRxDrI4XhV9yRXVV5cPDviCw1HxuLPQftWo+IbkrbapLcRC3S3MQVQ/wA3mDZ83yhDnjtyJbeyGkju/DviCHxLprXttY6hZxh9gW/tWgd+AdwVuSvPX61rVmeG9GHh3wvpmjLMZxYWsdv5pGN+1QM47dOladW7X0JV7ahRRRSGFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABTDNGJlhMiiVlLBNw3EDAJx6cj8xT64n4kX0nhu1sPFttbSXEmmSPDLFEMvJFOuwKP+2ogP0U0noNK52C3tq8IlS5haNn8sOJAQX3bdufXPGPXisa38X2B1HTNMvpILbUtRtXuUt1uY5ANpUFQwPzE7uMDkK3pXDeFtEudL8W2fgydZpbPSWGuG4c7lkZ4hGFJPOTcGeT22rj2teH5tNsb3wI2oPa28k2i3MEDTFVLyb7baik9W+9gDnrTejt6/l/noCV0/6/rQ6/w34x0vxFp+nMt3aQaje2iXR077UrzRqwz93gkD1wK25riG2j33EqRJnG52Cj9a8u0XSLC0+Efg24tLKGKcXumz+akYD75J4xI2euSHYH2OK6/xzYW2p6bptnfwrPbyara74nGVcB84I7jI6U7bebt+X+YpaX9L/n/kX/8AhLvDf2GG9/4SDS/sk8vkxT/bY/Lkf+4rZwW9hzVnUtb0rRoo5dY1OzsI5W2RvdXCxB29AWIya5G28P6TL8RPFs0ulWru+m2sZZoFO5WEoYdO+1QfXaPQVxmnXX9n3ulXWsa1o+kpP4XsEtZ9dsTOkqhG85I382MBslCy8lsr6UW0v6fjf/L+rD/r8v8AM9j1HWNM0izF3q2o2ljbEgCa5nWNCT0+ZiBSy6rp0Fkt5Pf2sdqyGRZ3mUIVAyWDE4xjnNeQ2l3b6RD4ciu73S9L8qwuZbPVNfsnRBE8/EMNuZgFOzZjLlwm0YGSBN4Wg0/V7Xwfa3CwXy22vak0kbWvlKjjz5EJhbJj4ZGCnoCtFv6+dv6/pi2PWrLUrHUrJLzTry3u7WT7k8Equjc44YHB5qzXFWkGi2WpeKV1KGCOxbVbRyjJ8nnGODYdo7+Zs59ea7WldMLNBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRgHqOlFFABRRRQAUEA9Rn60UUABAPUZooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAopNy79u4bsZxnnFLQAUUVwuo+J73SPiJuu7r/im5B/Z8isqjyLwReeH3YztZPl6n5qAO6orL8NLfL4bsW1aeWa9liEsxm27kZvm2fKAPlzt6dq1KbVnYAooopAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHA+G7C30/4zeKUthJiXTrKVzJM8hLF5s8sSfw6DtXfVx2l6L4jt/ibquuXdvpY02+t4rZRFeSNMixFyrFTEFJbfyN3Hqe/Y0o6RXz/Ng/if8AXRGXrPiXRPD3k/29q1np3n58r7VOse/GM4yecZH51xVzq3w21Dw/daPq3jLTtQtrq7N3Ibi+g3bvMEgGVAG0EAAEfd4zium8eeC7Dx54TudG1ABGYb7efbkwSgfK4/kR3BIrxD4O3WlaL4ovPh34/wBC01tQinZbO4ubONmZupjLFckMPmQnrnHdRTjdyt1HLSNz3G0+IHhC/vIbSy8TaVcXEzhIoorxGZ2PAAAPJroa4LxF8MdFn1bQ9Y8P6PZWOoabqUM7taxLCJItwDhsABsDkd/lwOprqtd1220CxSe4SSaWeVYLa2hAMlxK33UQEgZ4JySAACSQATT05b/10F1NOimoWaNS67GIBK5zg+lK5KoxRdzAcLnGT6UgForN0PXLbXrF57dJIZYZWgubaYASW8q/eRgCRnkHIJBBBBIINaVABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFfUL2LTdNub65YLDbQvNIx7KoJP6CvItP8V6pPZ+A9CuNXuF1fVpf7Y1OYSY8m0+aQRsT91Wyqj2U+vPoPxA0zUdb8F3ekaREXm1FktZX3qvlQu4Er8kZwm7gZJ9KrWPwu8G6dNps1rokQl0tme1d5Hchjj5myx3kbVwWzt2jGMCnHe7/r+tPxB7WX9f1qdDrEpg0O+lW6azZLeRhcpD5rQ4U/OEwdxHXbg5xivIP+EpvP8Aoqut/wDhGn/4xXtdFIfQ53wPfPqHhlJpdan1tvNdTd3Gnmzc89DHtXp645q7qmr3un3SxWnh3U9UQoGM1pJbKqnJG0+bMjZ4zwMcjnrjVqpq06Wui3txNcvaRxW7u9wm3dEApJYbgRkdeQR7USaSuKKbdjI/4SXVf+hI17/v/Yf/ACTWrpd9cahatLd6Vd6W4cqIbtoWZhgHcPKd1xzjk54PHTPm3gbxxrN54F8R2/ja6mstd0OJp55kSISiF4/NjcKFMecZGNpHAyMmu+8Jw6rB4V08eIb2S91N4FkuZXREw7DJUBFUYB4HGeKdv0/EVzYrwv8AaH8P2N/eeH59IMg8XT3Ags4bYfPOgOck5G3Y2CG7ZP1HrvinxLp/hDw3d61q8my3tkztH3pG/hRR3JPH/wBavK/hzrHh+81i58feN/E2iRa9qK7bSzl1KIf2dbfwoAW4Yjr3wexLVO79P6/Eq9l6nr2ixahBoVjFrU8dxqCW6LcyxjCvIFG4gemc1yerFp/jj4ehnUGCDSbueDJ4EpeNCQPUKcfRjS+J/iz4S0Pw5eX1n4g0vULuOJvs9ta3aTPLJj5VwhJAzjJ6CtLWtBvtVtdH1W1lhi8QaXiWJ2ysUu5QJYWxkhHHfkqQp5xgt3cub+tb/kLaNv60t+ZzPxAvbmw+IXhmDS/EN3p8mqSPaXcSu0kccbjCSBD8iSFlKoxHJJ4YKRXX6bNFpU+m6DaX41H/AEd5XlvL/wAy6MYI2vjGZAS2C2RjjrVQ/Dzw1Nos+n3Gl/uru4W6nH2mUv5qncCJN24AHOACAMnjk1La6Bc2PjKTUbZLVdNi0qKys7dCVMRV3ZlCgbVUjyxnn7gGOKForP8ArT/P+ugPV3X9a/5GZpRMHxs8QQwcRXGk2dxMo6eaHlQH67QB9APSu2rA8NaDPps1/qmrSRy6vqkiyXLREmOJVGI4UJAJVRnkgFiWOBnA36Oi/r+uwdWFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVyPxHvGXw4mlpZX90NUnjt5zZ2UtwI7cuvnM3lqcfJuA7knjocddRR1A8x8eeDdQv/AIhaFqOjBltNTA07WlVCwa3RvOXI6DOxk3H+8o716dRRR0sHW5w/xH+H2ieOYbL/AISbWdQsLW2kCRRW9xHFG8rsFUnehyxJCjnvgdTXI/8ADL/gz/oKa9/4EQ//ABqu+8V6lNb+IPDOnW901v8Abbx2lURq3nRxxs2wBgTneYzxggBjnANYI1nxlxnTfEX1Nhp//wAk0oobuc5Yfs9eALbxFHBDreqT31oUuXs3uoGIUNwXUR5CkjHbPOK9prkY760/4XBJam5h+0nRE/c7xv4mYnjr0INddTTbivn+dv0Fa0n8vyv+oUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRXP+MNcu9C06CW0k021Waby5b7VJxHb2q7SdzDcpckgKFBGSeoAoA6CivOrT4h6jd+Gbi605dJ1e8g1mHS457WZktbkyCM7wfmKY8zBGW5U4Jq9ceLdc8Px6tB4gtbK/u7W2gntDp4aFbh5pGiSIq7MVO8AbskYbOBij+vwv+THa/8AXnb8zt6K4C4k8Tp428JweI102WGS6nlSewDx+W4tZR5TK5O7IYkOCPun5RWt448TXPhyHThbzWVil5cGKXUtRRmt7UBSw3hSvLEbRllGe/QFvRIR1NFefz6z4m1D/hE57e40yET6rJFLJbytLBeRiCZldCj/AHGVSdrZIYL1xzBrnxDvdH1qdJr7w7bpBeLbrpE1zvvrhCwXzAVfCE53BSh4xkjPAHS56PRXD3niLxXc6h4hGiW2kpa6HP5f+lGRnvP3CSlBtIEZ+fG47uv3eObUviHW9a1VLHwtHY2wjsYb25uNQR5QPN3bIlVGXnCElieOODnhLX+u4HXUV5tpniPVNM8GPdyppemXM+tXkV3c6jdgWtkRNJlsko0mWUBVG0ndyRiprP4g6jeeE76901NL1e9tNWg0yKa0lZbW6MhhG8H5igHnYIy2Cp5NEbv8Pxt/mD0b+f4X/wAj0OiuOGreLL3Vn0nTv7GS40+3ifULuaOVo5JXyRFGgYFflAJZmONw+U1Y+HM1xceEWmvoFtrl9RvzNCr7xG/2uXKhsDIB74FOwdDqaK828Ra94n1XwX4i1XTLXTf7Hhhu4Etpd4uJ0j3xvKJAdq8qxVdpyAPmGeJNW8fPYaz/AGLZ6t4e0g2VnDJLNrc+0zu65EcaBlwAACXycbgNp5qb/wBff/kO39fd/mei0Vxfhv4gJrMlvLeww2lpd6Wb+CQSbxmJylwm7oQhKEMMZDZo0jX7u/kt9Un0lBeXGiyXkUKSFXKGTMcR3EKCVK5J6HPOKfX+vP8AyYun9eX+Z2lFed6D48v9U8QQaT/a/hnUZ722lkSPS5WkNk6AECQhyJF5xkbDx054Xw9q93a+GvDF/wCJngu5vs88zXSGTckawliTuY7mwCCT+GKTaWrHa+x6HRXDW3ibxRbWel61rdlpyaXqU8ERtLff9osxOwWNmkJ2yfMyhgFXGTgtjmGPxZ4qm06511bPSl0qz1Ca1e2JkM88UdwYjIr5CoRgnaVbOOozxVmTdbnf0VXt5Lt7m6W5gjjhSQC3dZNxkXaCSwwNp3bhjngA98VYpDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK5/xL4dutXv9K1HTL23tL7TJJHiN1afaImDptbKhlIPTDBhjnrmugooA4y18DagHuW1PX1vWuNUttUZhZCMrJFsBQYfGwiNAB1XByWJzWnrfhKHXLi/knu5IlvLKK2UxLh4XjkaRJVY9wzAgY/hroKKP6/C35Du/wCvW/5nKW3hjXZ9e0rU9f8AEMF5/ZjOY4LWw8hJS8bIWfMjndzwRgAZGOcjU13StRv2tJ9G1X+z7m2djtliM0E6sMFZIwy57EEMCCPciteih6iWhxVt4EvbCxtjYatax6hHqz6pJIdP/cM7xNEyLEsgKja+fvk5GTnNQXPgDWDpuoaVp3iO3s9NvLqW7GNNDTh5JDKVeTzMMm49NoJXjd3rvKKP6/r7gev9f13MOw8OvaR67514ssmsTec7LDtER8iOIgDcc/6vd1747ZOe/hLVLK/t77w7rkVnOLCKxukurIzwziPOyQKHQq43N/EQQcEcZrrKKP8AgfgBw8fw+vbO20ySx1xJdS0+9urtbm/sRMspuCxfcismGBbhlIxyMYNTWngjUFW9/tPXlvWvNTtNTZhZiMrJC0ZKDD42ERIAOq85LE12VFOL5Wmun6f8MD1Ob1Hw7qw8Qzav4c1qHT3u4EhuoLqyNzG5TOyRQJEKuAxB5IIxxxVzwtoLeG9BXTpL6S/cTzzNcyoFeQyStISQOM5fqMZ9B0rYopdLAcPe+A9Xm0zVdEsPEwtdE1J53aJrEPcQCYlnRJd4GwszHBQkAkAjgi/deF9XttWmv/DWt29i93bxw3Ud3Ym4RmjG1ZEAkQq2Dggkg4HHHPU0UrDvc4/Xvh9Fr3hvTNKuNWvC9lJ+9vJW3y3UTKVmjc5HEisRx04wOAK0dd8KQ68t7FNcNDDeaXJpzLGgyqufvAnjj0xW/RT/AK/Cwv6/U5G08LeIBrGlX2oeIbORNNLItra6X5MUiMm05BlYh+mCDgcjbzT9L8Gz2+n6dZarfwXsGniWKNY7Ux+ZC8ewK+XbLDJywwD6DrXV0UNJ7gtNjjbTwVqoi03TtV8QrfaNpk0csEAs/Lnl8ogwiWXeQwUhT8qKWKjPcHQh8J+V4RvNE+25+03NxP53lfd82dpsbc843YznnGeOldFRTuxWRXt4bqO6u3uLoTQySBoIhEF8lQoBXOfmywLZ4647VYoopDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKK4W08V+MNYm1FtE8O6VLa2d9PZrJcao8buYnKlioiOM49aA8zuqK5UX3j4oCdA8PhsdP7Ym6/+A9AvvH2znQfD+7H/QYm/wDkegDqqK5Vb7x7tG7QfD+cc/8AE4m/+RqUX3j3HOg+H8/9hib/AORqAOporlVvvH235tB8P5/7DE3/AMj0ovvHvfQfD/8A4OJv/kagDqaK5Vb7x9zu0Hw/14/4nE3/AMj0LfePud2g+H+vH/E4m/8AkegDqqK5b7d495/4kPh//wAHE3/yNSC+8fbjnQfD+O3/ABOJv/kegDqqK5b7d49z/wAgHw/j/sMTf/I1J9u8fb/+QD4f24/6DE3/AMj0AdVRXK/bvH2//kA+H9uP+gxN/wDI9L9u8e5/5APh/H/YYm/+RqAOporA0268XSahEuraRo9vZnPmS22pyyyLwcYUwKDzjuP6Vv0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVx3w3dX0/XtjBtviG/Bwc4PnGuxrkPhz/AMg/Xf8AsYL/AP8ARxpx3fp+qB/CvX9GdfRUV1C9zaSwx3EtszqVE0O3enuNwIz9Qa5H4VzXE3gpjeXdxeSpqN5H51zIXkYLcOoyT7AVPWwdLnZ0UVQ1cPc6Vd2llqEVjdyxNHHcMofyWIwG25GcZzjND20GtzjNM8b6xqGn6pastrFqkd3B/Z0ghZ457S4kCwzFd/PActhhjbn2r0EZCjccnHJx1rmD4X0ttf0PVzdRpPo9q9sqRHakqlQq5G48L82Ac43GulSaKQ4jkRz1wrA1Wgh9FFFIAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAorH8W+JLbwj4Uv9dvkaSKzi3eWnV2JCqvtliBntmvOzrXxFPifw3Y6vq+naQ3iFJ5EtLfTxObUxqH8tmZxklDyw6MMYI5oWrsOztc9coqrqk8tro95PbzW0EsUDukt2SIUIUkM5BGFHU89M15b/AMJ/4i/6Hj4Yf+BMv/x6gLaXPXKKw/CGqXWseH0ur/UdH1GYyMpuNFcvbkA8AEknI781uUCCuP8AhwCNP17Jz/xUGoY46fvjXYVxvw2kEmn6+AGG3xDfg5GP+WxP9acd36fqgey9f0Z11zK8FrLLFBJcuillhiKhpCP4RuIXJ9yB71yHwxs9X0zw7c2Wu6NPpkwvbi4TzJ4ZFkWWV5Bgxu3IDYOQPbNatx4x0y1upbeS11pnicoxi0K9kUkHBwyxFWHuCQe1R/8ACcaV/wA+mvf+E9f/APxml1uHSx0VeHftB/DAazpr+L9EgH9oWcf+nRoOZ4QPv+7KPzX/AHQK9psL6LUrGO7t0uEjkztW4t5IHGCRykiqw6dwMjnoasEAggjIPUVMlcadmeGfC3wl8MfiH4SS8/4Ru2j1K2Aiv7dbiYbHxwwG/wC62Mj8R2rqfCPwysfAnxTur3w9bSxaTfaUy7GYusEqyp8oZiT8w5AJPRvauAXw9d+A/wBpW0s/ADxzW+pIJr3TlyFtYGPzh+ygY3L3HAxyN30TWile015/5E8trwfl/mUbvWLKz1Oz06WRjeXm4wwxoWYqo+Zzj7qjIG44GSB1IFXq4fwS51Hxx411K6VjcQ6gmnRF/wCCCOJWCr6AtIze+6su01LxVqHxF13wpa+JrdYbIW94LtraKSeJGHMAQALyeSzZIUgdWDCIu9vP+vyG9L+X9fmemUVXgu2mvLmA200awFQsz7dkuRn5cEnjocgc+tY+v+KptCvo7eLwzrmrB49/nadBG6LyRtJaRTnjPTuKYHQUVy+keNJ9V1WGyk8I+I9PWXObm9toliTAJ+YrISM4x06kVJ8QNduvDngi+vtMwdQOyCzBUNmaRwicHry2ce1DA37q4W0s5rmRJHWFC7LFGXcgDOAo5J9hyayL918Q6Db3Wka+1hp8q+fJfWhTc0e0nCs6lVGcEkjIAI46jI0/xNe6j8U5/DtrcJLZ6PpivqThBlrqQjYuccfIC3HrjtWLpuhnWfEnjHw0mp32m2NnqltfxfYWQHdLEJHQh0YbDIC5GOSTngkEs3/Xnb+v+CPb+vK51/gqbU7jwlaS61LJNcsZNs0sYjeWLzG8p2UAYZo9hIwOT0B4reqjpGmtpdj5EuoXmoyFy73F4ymRif8AdVVAAAGAB+ZJN6m9xBRRRSAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMvxH4fsvFPh660bU/NFtcqAzQvtdCGDKyn1BAPcccg1V03wpb2etDWL++vNW1JIfIiub0p+5jOMqiRqqrkjJONx6ZxgVvUUBurDZI0ljaOVFdHBVlYZDA9QRVL+wdI/6BVj/AOAyf4VfooAit7W3s4vKtII4I852RIFGfXArlPiJonifWvD13H4X19tKZLZ2EcEH72eQchfN3fIp6fKM55zjg9hRSkroadmcL8HPFL+LPhjpt1cytLe2oNpdM7FmLpwCxPJJXax+tW/hz/yDtd/7GC//APRxri/Af/FFfHTxP4Rf93Zawo1SwUnAycllUfiw+kddp8Of+Qdrv/YwX/8A6ONXe75u6/VX/Elqy5ez/R2/Af8AEnxDq/hfwZNqnh2OC4v45Y0itZrd5vtG5sbVCMpB53Z54U8dxTvfGV/qngnw/rPhOWyS41m4ghSK7gaZcv8A6wfK64KAOx6/cI96k1fWdM1D4qaF4eN/b/aLGObUJLcyru8zy/LjTb3OySR8dcKD0rmPBfh6/wBI+Kl/4ekjX+wdFkk1bTj/AHTdAoqAdlXFwB7mpj59dfu3Xz1+4qWn9d9v0+89aQMI1EhDPgbiowCfpziuV+IfjiDwP4d+0rGbrU7t/s+nWSAlriY8AYHOBkE/gOpFdXXkHxB+COo/EDxU+rXvjE28aqEtbQafuFug7A+aMknknHP4AUnd6ArI6j4aeCJvC+m3Gpa9N9s8S6u/n6ldMQSCeREv+yvtxn2AA2tc8UW2kazo+koY5r/Vbnykg8zDLGFZnkx1wAv5kCvFP+GU/wDqcv8Ayl//AG6un+HXwFh8C+MYtdk8QjU3t43RIRZeVtZlxnPmN/CTxjvV6N6/15E6pf1951174av7bxRqUumS3MWneIUQXktnIqT2Nwi7VmQtwVZQFIAJBCnBBJEsXw30eDWdN1OG4v0uLKNkkKz4+2lpBKWnIGWPmDdwQCeCCOK66ipWjT7DOa8F6Xeabb6vJqNktrcXuq3NydrKfMQviNsLwP3ap15znOK6WiijyAK4b4haJeeK9Y8OaFA1/aWa3TahdX9oNph8lf3YDkFQxd1IBz90ntXc0UAcXo/w80fwdrGp+INITUru6uIg32M3IYPIq4LDcRukfnLux5ZuQCa0/COhXGlQ39/qvlnVdXuTd3giOUjO0KkSnuERVGe5yeM4roaKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPIfjhBLoOoeGPH9krGTRL1YrrYBl4HPIP6r/wBtK6v4ZTxXWi6xcW7rJDNrt9JG6nIZTMSD+VbPjDw/H4p8Haposu3/AEy3ZEZhnY+Mo34MAfwriv2e4Zbb4TwwXCNHLFe3COjDBVg+CD+NENOZf1q1/l+IT2T8/wAk/wDP8D0+jHOe9FFABXE6prksHi/VpY47N/7I0+FYpDaSzTB5pAZExHuZlCrESFXgspPArtqgisLSG9nvIbWGO6uAqzTrGA8oX7oZupxk4z0zQPTqchofjS81TW7azlRQkrEE/wBk38PRSfvyRBB07kfnVvwNBDbXXimK2jSKNdclOyNQACYYSeB6k5/GurqpY6Tp2mSTvpun2to9w++ZreFYzK3qxA5Puaadn8rfin+gnt/XmW6KKKQBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABVax06z02OVLC3jt1mmeeRYxgNI5yzfUnmrNFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//2Q=="
    }
   },
   "cell_type": "markdown",
   "id": "8e282d72",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 3\n",
    "\n",
    "## Linear models for classification problems\n",
    "\n",
    "Classification — The act of dividing an input space into a collection of labelled regions using decision boundaries. \n",
    "\n",
    "Linear Approach — Use of a straight line to partition a dataset. Intuitively this is easiest in terms of both describing the partition (“If it falls on one side of the line class as A, if it falls on the side class as B”) and performing the cut [1]. \n",
    "\n",
    "An example of discriminative training of linear classifiers is Logistic Regression [2].\n",
    "\n",
    "Logistic regression—maximum likelihood estimation of $\\vec{\\mathbf{w}}$ assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.\n",
    "\n",
    "This assignment has 2 parts as follows:\n",
    "\n",
    "**Part 1.** You will implement your own classifier for **Logistic Regression** (50 points) for 2 class classification using the dataset provided for Part 1 (pls. check the shared files). Check the performance of this algorithm with the classification accuracy.\n",
    "\n",
    "**Part 2.** You will implement your own **Multinomial Logistic Regression** (50 points) classifier for multiclass data. The multinomial logistic regression algorithm is an extension to the logistic regression model that involves changing the loss function to cross-entropy loss and predicting the probability distribution as a multinomial probability distribution to natively support multi-class classification problems [3]. Check the performance of this algorithm with the classification accuracy.\n",
    "\n",
    "**_Preprocessing the datasets_**:\n",
    "\n",
    "**Encoding**\n",
    "\n",
    "Machine learning models require all input and output variables to be numeric.\n",
    "\n",
    "This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model. You can use One-Hot Encoding is available in the scikit-learn Python machine learning library via the OneHotEncoder class[4]. \n",
    "\n",
    "**Data standardization [5]**\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range. One of the most popular techniques for scaling numerical data prior to modeling is standardization. \n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values ($\\mu$) is 0 and the standard deviation ($\\sigma$) is 1. A value is standardized as follows:\n",
    "\n",
    "![standardization.JPG](attachment:standardization.JPG)\n",
    "\n",
    "This can be thought of as subtracting the mean value or centering the data.\n",
    "\n",
    "**Important Remarks:**\n",
    "\n",
    "You are provided the necessary preprocessing codes as separate notebooks for both datasets that you are going to work with in this assignment. Please check the shared notebooks first before beginning to work with your own algorithms. Note that, in these notebooks, you are also provided with the binary and multinomial logistic regression performances of  of the sklearn library as a baseline for your own implementations. You will be implementing your own solutions that comprise of the necessary training procedures and the codes to train your models for each part before submission.  \n",
    "\n",
    "**Steps for all parts:**\n",
    "\n",
    "- Check the data has any null values or not. \n",
    "- Check if in the data, some of our columns have numeric values and some of them have categorical values.  To work with categorical variables you need to encode them to prepare this data for the machine learning algorithms.\n",
    "- Split your data into train and test sets with a ratio of 80:20 with seed=1.\n",
    "- Apply the data standardization technique to the datasets to standardize the input variables.\n",
    "- Build your models with your own implemenation (with proper comments and explanations) using the training set.\n",
    "- Report these classification models according to their accuracy over the test set. Comment on their classification performance.\n",
    "- Finally, plot confusion matrices for your models over the test set, and comment on the outcomes.\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://towardsdatascience.com/classification-a-linear-approach-part-1-b080c13992dd\n",
    "\n",
    "[2]: https://en.wikipedia.org/wiki/Linear_classifier\n",
    "\n",
    "[3]: https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n",
    "\n",
    "[4]: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "[5]: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ccd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here for data loading and preprocessing (You can enlarge the cell as much as you want.)\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.preprocessing import (FunctionTransformer, StandardScaler)\n",
    "\n",
    "from collections import Counter\n",
    "#data reading\n",
    "data1 = pd.read_csv('Part1_Invistico_Airline.csv')\n",
    "\n",
    "#Prepare data 1\n",
    "data1.isna().sum()\n",
    "data1['Arrival Delay in Minutes'] = data1['Arrival Delay in Minutes'].fillna(data1['Arrival Delay in Minutes'].median())\n",
    "\n",
    "\n",
    "def object_cols(df):\n",
    "    return list(df.select_dtypes(include='object').columns)\n",
    "\n",
    "def numerical_cols(df):\n",
    "    return list(df.select_dtypes(exclude='object').columns)\n",
    "\n",
    "obj_col = object_cols(data1)\n",
    "num_col = numerical_cols(data1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in obj_col:\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    \n",
    "norm = Normalizer()\n",
    "data1[num_col] = norm.fit_transform(data1[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebb8f4",
   "metadata": {},
   "source": [
    "# Prepare the data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456557f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.084334</td>\n",
       "      <td>-0.840893</td>\n",
       "      <td>0.365244</td>\n",
       "      <td>1.619659</td>\n",
       "      <td>0.386817</td>\n",
       "      <td>0.997769</td>\n",
       "      <td>-0.959211</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>-0.622021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.518803</td>\n",
       "      <td>0.455155</td>\n",
       "      <td>0.384185</td>\n",
       "      <td>-1.779009</td>\n",
       "      <td>0.016673</td>\n",
       "      <td>-1.292021</td>\n",
       "      <td>0.133636</td>\n",
       "      <td>0.189691</td>\n",
       "      <td>1.758817</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.203864</td>\n",
       "      <td>0.676431</td>\n",
       "      <td>-0.809066</td>\n",
       "      <td>0.230638</td>\n",
       "      <td>-1.146637</td>\n",
       "      <td>0.133698</td>\n",
       "      <td>1.273999</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>1.064406</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.173942</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.280012</td>\n",
       "      <td>0.230638</td>\n",
       "      <td>-0.670737</td>\n",
       "      <td>0.220105</td>\n",
       "      <td>-0.210848</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>-0.622021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.143280</td>\n",
       "      <td>0.312905</td>\n",
       "      <td>-0.989000</td>\n",
       "      <td>0.939925</td>\n",
       "      <td>-0.811745</td>\n",
       "      <td>0.306512</td>\n",
       "      <td>0.941393</td>\n",
       "      <td>0.547324</td>\n",
       "      <td>1.064406</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.118018</td>\n",
       "      <td>0.455155</td>\n",
       "      <td>0.696702</td>\n",
       "      <td>-0.242220</td>\n",
       "      <td>-1.199515</td>\n",
       "      <td>0.349716</td>\n",
       "      <td>-0.543454</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>2.552430</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-0.278105</td>\n",
       "      <td>-1.172808</td>\n",
       "      <td>0.431536</td>\n",
       "      <td>-0.124005</td>\n",
       "      <td>1.144731</td>\n",
       "      <td>0.522530</td>\n",
       "      <td>-0.329636</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>-0.622021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-1.093654</td>\n",
       "      <td>-0.366729</td>\n",
       "      <td>0.261071</td>\n",
       "      <td>0.673943</td>\n",
       "      <td>1.091854</td>\n",
       "      <td>-1.680853</td>\n",
       "      <td>-0.460303</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>1.262809</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.136660</td>\n",
       "      <td>1.166400</td>\n",
       "      <td>-0.894298</td>\n",
       "      <td>0.910372</td>\n",
       "      <td>0.051925</td>\n",
       "      <td>-1.983278</td>\n",
       "      <td>0.644424</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>-0.622021</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-0.012469</td>\n",
       "      <td>-1.157003</td>\n",
       "      <td>-0.354494</td>\n",
       "      <td>-1.542580</td>\n",
       "      <td>2.061279</td>\n",
       "      <td>-0.471153</td>\n",
       "      <td>0.763211</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>1.362011</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -1.084334 -0.840893  0.365244  1.619659  0.386817  0.997769 -0.959211   \n",
       "1    0.518803  0.455155  0.384185 -1.779009  0.016673 -1.292021  0.133636   \n",
       "2    1.203864  0.676431 -0.809066  0.230638 -1.146637  0.133698  1.273999   \n",
       "3    0.173942  0.012602  0.280012  0.230638 -0.670737  0.220105 -0.210848   \n",
       "4    1.143280  0.312905 -0.989000  0.939925 -0.811745  0.306512  0.941393   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "143  0.118018  0.455155  0.696702 -0.242220 -1.199515  0.349716 -0.543454   \n",
       "144 -0.278105 -1.172808  0.431536 -0.124005  1.144731  0.522530 -0.329636   \n",
       "145 -1.093654 -0.366729  0.261071  0.673943  1.091854 -1.680853 -0.460303   \n",
       "146  0.136660  1.166400 -0.894298  0.910372  0.051925 -1.983278  0.644424   \n",
       "147 -0.012469 -1.157003 -0.354494 -1.542580  2.061279 -0.471153  0.763211   \n",
       "\n",
       "            7         8  Type  \n",
       "0   -0.257351 -0.622021     2  \n",
       "1    0.189691  1.758817     3  \n",
       "2   -0.257351  1.064406     2  \n",
       "3   -0.257351 -0.622021     2  \n",
       "4    0.547324  1.064406     2  \n",
       "..        ...       ...   ...  \n",
       "143 -0.257351  2.552430     2  \n",
       "144 -0.257351 -0.622021     1  \n",
       "145 -0.257351  1.262809     2  \n",
       "146 -0.257351 -0.622021     6  \n",
       "147 -0.257351  1.362011     2  \n",
       "\n",
       "[148 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading part2 data\n",
    "data2 = pd.read_csv('Part2_glass.csv')\n",
    "X2 =data2.drop(['Type'], axis = 1)\n",
    "Y2 = data2[\"Type\"]\n",
    "\n",
    "#checking data types to look there is need any changes about the data types\n",
    "data2.dtypes\n",
    "#there are any non-numerical data types so there are no need to change\n",
    "\n",
    "#checking there are any null values\n",
    "data2.isna().sum()\n",
    "#there are any null value so we dont need to handle that too\n",
    "\n",
    "#handling  with the outliers\n",
    "#outliers: the value that is considerably higher or lower from rest of the data\n",
    "#Lets say value at 75% is Q3 and value at 25% is Q1.\n",
    "#Outlier are smaller than Q1 - 1.5(Q3-Q1) and bigger than Q3 + 1.5(Q3-Q1). (Q3-Q1) = IQR \n",
    "#We will use describe() method. Describe method includes:\n",
    "data2.describe()\n",
    "#We are define some sub dataset to clean data\n",
    "outlier_col_indeces = []\n",
    "# iterate over features(columns)\n",
    "for col in X2:\n",
    "    step = 1.5 * (np.percentile(data2[col],75) - np.percentile(data2[col], 25))\n",
    "    outlier_col = data2[(data2[col] < np.percentile(data2[col], 25) - step) | (data2[col] > np.percentile(data2[col],75) + step )].index\n",
    "    # append the found outlier indices for col to the list of outlier indices \n",
    "    outlier_col_indeces.extend(outlier_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "outlier_col_indeces = Counter(outlier_col_indeces)        \n",
    "delete_col = list( k for k, v in outlier_col_indeces.items() if v > 2 )\n",
    "    \n",
    "data2 = data2.drop(delete_col).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X2 =data2.drop(['Type'], axis = 1)\n",
    "Y2 = data2[\"Type\"]\n",
    "\n",
    "#We need to standartize our data \n",
    "trans = StandardScaler()\n",
    "X2 = trans.fit_transform(X2)\n",
    "# convert the array back to a dataframe\n",
    "X2 = pd.DataFrame(X2)\n",
    "data2 = pd.concat([X2,Y2],axis = 1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499e2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after  0 iteration is :  0.6931471805599453\n",
      "cost after  100 iteration is :  0.6001787565931624\n",
      "cost after  200 iteration is :  0.5814157435963802\n",
      "cost after  300 iteration is :  0.575475607687114\n",
      "cost after  400 iteration is :  0.5731583702971473\n",
      "cost after  500 iteration is :  0.5721327486499544\n",
      "cost after  600 iteration is :  0.5716354442637602\n",
      "cost after  700 iteration is :  0.5713732807942337\n",
      "cost after  800 iteration is :  0.571221447697325\n",
      "cost after  900 iteration is :  0.5711232470617238\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate Classification Ratio =  0.7464992060054858\n"
     ]
    }
   ],
   "source": [
    "# Code here for Logistic Regression (You can enlarge the cell as much as you want.)\n",
    "\n",
    "#We decide iterations and learning rate here\n",
    "iterations = 100\n",
    "learning_rate = 0.5\n",
    "\n",
    "\n",
    "#We split our train and test data\n",
    "#I choose %80 of the data with random selection method for train and then drop the selected values for test data\n",
    "train = data1.sample(frac = 0.8,random_state=42)\n",
    "test = data1.drop(train.index)\n",
    "#Splitting X and Y values \n",
    "X_train = train.drop(['satisfaction'], axis = 1)\n",
    "X_test = test.drop(['satisfaction'], axis = 1)\n",
    "Y_train = train['satisfaction']\n",
    "Y_test = test['satisfaction']\n",
    "\n",
    "\n",
    "#Changing frames to their values so we can reshape them\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "#Reshaping the data so we can do linear algebra without any problem\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.reshape(1, X_train.shape[1])\n",
    "\n",
    "X_test = X_test.T\n",
    "Y_test = Y_test.reshape(1, X_test.shape[1])\n",
    "\n",
    "#I am predefine sigmoid function so we can use it like a util\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Our logistic regression model\n",
    "def LogisticRegression(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    #m is number of items in the dataframe\n",
    "    m = X_train.shape[1]\n",
    "    #n is number of features\n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    #In the beginning fill our weight array with zeros\n",
    "    W = np.zeros((n,1))\n",
    "    \n",
    "    #We initilize value 0 for start\n",
    "    B = 0\n",
    "    \n",
    "    #I am defining a cost array so we can see cost changes step by step\n",
    "    cost_list = []\n",
    "    \n",
    "    #With iteration we can choose how many step we want\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #A = sigmoid(W.T * X + b)\n",
    "        A = sigmoid(np.dot(W.T, X) + B)\n",
    "        \n",
    "        # cost function\n",
    "        cost = -(1/m)*np.sum( Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "        \n",
    "        # Gradient Descent algorithms works in here\n",
    "        #dW = dcost/dw\n",
    "        dW = (1/m)*np.dot(A-Y, X.T)\n",
    "        \n",
    "        #dB = dcost/db\n",
    "        dB = (1/m)*np.sum(A - Y)\n",
    "        \n",
    "        #We are updating the W in here\n",
    "        W = W - learning_rate*dW.T\n",
    "        \n",
    "        #We are updating B in here\n",
    "        B = B - learning_rate*dB\n",
    "        \n",
    "        # Keeping track of our cost function value\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        #in some steps our algorithm shows us the cost values\n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"cost after \", i, \"iteration is : \", cost)\n",
    "            \n",
    "        \n",
    "    return W, B, cost_list\n",
    "\n",
    "\n",
    "W, B, cost_list = LogisticRegression(X_train, Y_train, learning_rate = learning_rate, iterations = iterations)\n",
    "\n",
    "#Plotting the relationship between cost and iterations\n",
    "plt.plot(np.arange(iterations), cost_list)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def AccuracyTest(X, Y, W, B):\n",
    "    \n",
    "    true_detected_count = 0;\n",
    "    A = sigmoid(np.dot(W.T, X) + B)\n",
    "    \n",
    "    #We deciding a treshhold value so if the value is bigger then this value it take value 1 else 0\n",
    "    A = A > 0.5\n",
    "    \n",
    "    #A is a boolean value now so we transform it to an int\n",
    "    A = np.array(A, dtype = 'int64')\n",
    "    \n",
    "    #We calculate the accuracy value\n",
    "    accuracy = (1 - np.sum(np.absolute(A - Y))/Y.shape[1])\n",
    "    true_detected_count = (Y.shape[1]*accuracy);\n",
    "    print(\"Accurate Classification Ratio = \",accuracy)\n",
    "\n",
    "#We are using test datas to calculate accuracy value\n",
    "AccuracyTest(X_test, Y_test, W, B)\n",
    "\n",
    "\n",
    "#We have 0.74 accuracy which not perfect but some testes i made we cant increase the accuracy more.\n",
    "#smaller learnıng rate or more iterative processes wont give any progress for us on this ata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b40b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "Model is training for class  1\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.5128542109862932\n",
      "cost after  400 iteration is :  0.4940237376738528\n",
      "cost after  600 iteration is :  0.48730985918724135\n",
      "cost after  800 iteration is :  0.48389473305396735\n",
      "cost after  1000 iteration is :  0.48176657555996394\n",
      "cost after  1200 iteration is :  0.4802772522142651\n",
      "Model is training for class  2\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.5082273520877489\n",
      "cost after  400 iteration is :  0.4891026741048816\n",
      "cost after  600 iteration is :  0.48236574948823047\n",
      "cost after  800 iteration is :  0.4790262218656026\n",
      "cost after  1000 iteration is :  0.4770314829610294\n",
      "cost after  1200 iteration is :  0.47570436414230227\n",
      "Model is training for class  3\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.3066192740279702\n",
      "cost after  400 iteration is :  0.2751389751903517\n",
      "cost after  600 iteration is :  0.2610538356999146\n",
      "cost after  800 iteration is :  0.25120052449009456\n",
      "cost after  1000 iteration is :  0.24341245523545518\n",
      "cost after  1200 iteration is :  0.2369770837023617\n",
      "cost after  1400 iteration is :  0.2315458120469836\n",
      "cost after  1600 iteration is :  0.2269039239479078\n",
      "cost after  1800 iteration is :  0.22290009658664364\n",
      "Model is training for class  4\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.10684352655318179\n",
      "cost after  400 iteration is :  0.05357903152953079\n",
      "cost after  600 iteration is :  0.035393611648026006\n",
      "cost after  800 iteration is :  0.02634076858423285\n",
      "cost after  1000 iteration is :  0.020946519629743306\n",
      "cost after  1200 iteration is :  0.017373406566442453\n",
      "cost after  1400 iteration is :  0.014835317476322234\n",
      "cost after  1600 iteration is :  0.012940751929975366\n",
      "cost after  1800 iteration is :  0.011473171629825104\n",
      "Model is training for class  5\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.13561077401236216\n",
      "cost after  400 iteration is :  0.0794350359224202\n",
      "cost after  600 iteration is :  0.05854089218469657\n",
      "cost after  800 iteration is :  0.04743724845542674\n",
      "cost after  1000 iteration is :  0.04044785810093004\n",
      "cost after  1200 iteration is :  0.03558868253609108\n",
      "cost after  1400 iteration is :  0.031982588041782195\n",
      "cost after  1600 iteration is :  0.029180327120459034\n",
      "cost after  1800 iteration is :  0.026927162094107476\n",
      "Model is training for class  6\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.1611921000622571\n",
      "cost after  400 iteration is :  0.10307566835854712\n",
      "cost after  600 iteration is :  0.07963021403959468\n",
      "cost after  800 iteration is :  0.06656434372736751\n",
      "cost after  1000 iteration is :  0.05810098692271405\n",
      "cost after  1200 iteration is :  0.05210218786854692\n",
      "cost after  1400 iteration is :  0.04758493207796767\n",
      "cost after  1600 iteration is :  0.0440320758512379\n",
      "cost after  1800 iteration is :  0.04114488759158984\n",
      "Model is training for class  7\n",
      "cost after  0 iteration is :  0.6931471805599454\n",
      "cost after  200 iteration is :  0.1280740889526361\n",
      "cost after  400 iteration is :  0.07944882953280502\n",
      "cost after  600 iteration is :  0.062342344155961925\n",
      "cost after  800 iteration is :  0.053287996605432146\n",
      "cost after  1000 iteration is :  0.047402873242365993\n",
      "cost after  1200 iteration is :  0.043086725692409775\n",
      "cost after  1400 iteration is :  0.039675069817783584\n",
      "cost after  1600 iteration is :  0.03684783881845019\n",
      "cost after  1800 iteration is :  0.03443365987193918\n",
      "---------------------------------------------------------------------\n",
      "accuracy rating is :  0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Code here Multinomial Logistic Regression (You can enlarge the cell as much as you want.)\n",
    "\n",
    "\n",
    "#We split our train and test data\n",
    "#I choose %80 of the data with random selection method for train and then drop the selected values for test data\n",
    "train2 = data2.sample(frac = 0.8,random_state=42)\n",
    "test2 = data2.drop(train2.index)\n",
    "#We create 7 dataset for 7 different classes later on I will explain why we create 7 dataset\n",
    "D1 = train2.copy();D2 = train2.copy();D3 = train2.copy();D4 = train2.copy();D5 =train2.copy();D6 = train2.copy();D7 = train2.copy();\n",
    "#Lets put them into an array\n",
    "datasets = [D1,D2,D3,D4,D5,D6,D7]\n",
    "#In here we are using our datasets as a one class classifier and for all the datasets We are setting type 1 or 0 \n",
    "#If and data belongs to class 2 and we are in D2 we are setting type 1 else type is equal to 0 and we are doing this for all the datasets\n",
    "for i in range(7):\n",
    "     datasets[i][\"Type\"] =[1 if k == i + 1 else 0 for k in datasets[i][\"Type\"]]\n",
    "        \n",
    "\n",
    "#We decide iterations and learning rate here we give a huge value to iterations2 because it find optimal value for itself and need space for that\n",
    "iterations2 = 2000\n",
    "learning_rate2 = 0.05\n",
    "\n",
    "\n",
    "#Train_datadict['key'][0] ==> X && Train_datadict['key'][1] ==> y\n",
    "Train_datadict = {}  \n",
    "\n",
    "X_test2 = test2.iloc[:,:9]\n",
    "X_test2 = X_test2.T\n",
    "y_test2 = test2.iloc[:,-1]\n",
    "y_test2 = np.array([y_test2])\n",
    "for i in range(7):\n",
    "    #Splitting X and Y values \n",
    "    X_train2 = datasets[i].iloc[:,:9]\n",
    "    X_train2 = X_train2.T\n",
    "    y_train2 = datasets[i].iloc[:,-1]\n",
    "    y_train2 = np.array([y_train2])\n",
    "    \n",
    "    Train_datadict['D'+str(i+1)] = [X_train2,y_train2]  \n",
    "    \n",
    "    \n",
    "#We keep all the weight and bias parameters for different classes    \n",
    "trained_parameters = []\n",
    "#We keep all the cost values for different classes\n",
    "cost_values = []\n",
    "\n",
    "#Our logistic regression model\n",
    "def LogisticRegression2(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    #m is number of items in the dataframe\n",
    "    m = X_train2.shape[1]\n",
    "    #n is number of features\n",
    "    n = X_train2.shape[0]\n",
    "    \n",
    "    #In the beginning fill our weight array with zeros\n",
    "    W = np.zeros((n,1))\n",
    "    \n",
    "    #We initilize value 0 for start\n",
    "    B = 0\n",
    "    \n",
    "    #I am defining a cost array so we can see cost changes step by step\n",
    "    cost_list = []\n",
    "    \n",
    "    #With iteration we can choose how many step we want\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #A = sigmoid(W.T * X + b)\n",
    "        A = sigmoid(np.dot(W.T, X) + B)\n",
    "        \n",
    "        # cost function\n",
    "        cost = -(1/m)*np.sum( Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "        \n",
    "        # Gradient Descent algorithms works in here\n",
    "        #dW = dcost/dw\n",
    "        dW = (1/m)*np.dot(A-Y, X.T)\n",
    "        \n",
    "        #dB = dcost/db\n",
    "        dB = (1/m)*np.sum(A - Y)\n",
    "        \n",
    "        #We are updating the W in here\n",
    "        W = W - learning_rate*dW.T\n",
    "        \n",
    "        #We are updating B in here\n",
    "        B = B - learning_rate*dB\n",
    "        \n",
    "        # Keeping track of our cost function value\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        #in some steps our algorithm shows us the cost values\n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"cost after \", i, \"iteration is : \", cost)\n",
    "            \n",
    "        #I decide a minimum cost value if the cost value reach that values training can be over in this way we can keep \n",
    "        #iteration value flexible it is needed because for every class it should has a different value\n",
    "        if(len(cost_list)>5):\n",
    "            if i%2000 == 0:\n",
    "              print('running @ ',cost)\n",
    "            if i%2 == 0:\n",
    "              if abs(cost-cost_list[-2])<0.00001:\n",
    "                if abs(cost-cost_list[-3])<0.00001:\n",
    "                  break \n",
    "    return W, B, cost_list\n",
    "\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "#In here we take the test data and calculate probabilities for all the classes and pick the max prob as their class\n",
    "#and then control it with the original result to calculate our accuracy\n",
    "def AccuracyTest2(X_test2, Y_test2,dataset):\n",
    "    Y_test2 = np.array(Y_test2)\n",
    "    #We define a variable to keep accurate predicts counts\n",
    "    accuratepredicts = 0                \n",
    "    for i in range(X_test2.shape[0]):\n",
    "        probabilities = []\n",
    "        predict = 0\n",
    "        #7 is the how many glass type we have\n",
    "        for j in range(7):\n",
    "            #I am taking glasses one by one reshape them for multiplication and calculate probability value\n",
    "            b = X_test2.iloc[:,i].values.reshape((9,1))\n",
    "            W = trained_parameters[j][0]\n",
    "            B = trained_parameters[j][1]\n",
    "            z = np.dot(W.T,b)+B\n",
    "            A = 1/(1 + np.exp(-z)) \n",
    "            probabilities.append(A)\n",
    "        #I am adding the index one because my copy dataframes start from 1 not 0\n",
    "        predict = probabilities.index(max(probabilities)) + 1\n",
    "        #Here we check if can we found the true class\n",
    "        if Y_test2.T[i] == predict:\n",
    "            accuratepredicts+=1\n",
    "            \n",
    "    return  accuratepredicts\n",
    "\n",
    "#We train models for all the glass type in here with using our copied dataframes\n",
    "for i in range(1,8):\n",
    "    print(\"Model is training for class \",i)\n",
    "    W ,B , cost_list = LogisticRegression2(Train_datadict['D'+str(i)][0] ,Train_datadict['D'+str(i)][1], learning_rate = learning_rate2, iterations = iterations2)\n",
    "    trained_parameters.append([W,B])\n",
    "    cost_values.append(cost_list)\n",
    "  \n",
    "print(\"---------------------------------------------------------------------\")\n",
    "    \n",
    "#We calculate accuracy here\n",
    "accuracy = AccuracyTest2(X_test2, y_test2,i)\n",
    "#We take the accurate predict count and divided it into all the variable counts\n",
    "accuracy = accuracy/X_test2.shape[0]\n",
    "print(\"accuracy rating is : \",accuracy)\n",
    "\n",
    "\n",
    "#We have 0.55 accuracy in here which is not so good but we know with sckitlearn we can take 0.64 accuracy \n",
    "#which is close 0.64 but a little bad thought but i couldn't increase this value more.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1416131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
